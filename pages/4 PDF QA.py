import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain_openai import ChatOpenAI

def init_page():
    st.set_page_config(
        page_title="Ask My PDF",
        page_icon="ğŸ¥º"
    )
    st.sidebar.title("Options")

def select_model(temperature=0):
    models = ("GPT-3.5", "GPT-4")
    model = st.sidebar.radio("Choose a model", models)
    if model == "GPT-3.5":
        return ChatOpenAI(
            temperature=temperature,
            model_name="gpt-3.5-turbo"
        )
    elif model == "GPT-4":
        return ChatOpenAI(
            temperature=temperature,
            model_name="gpt-4o"
        )
    
def init_qa_chain():
    llm = select_model()
    prompt = ChatPromptTemplate.from_template("""
    # ä»¥ä¸‹ã®å‰æçŸ¥è­˜ã‚’ç”¨ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    ä»¥ä¸‹ã®å‰æçŸ¥è­˜ã®æ­£èª¤åˆ¤å®šã‚’ã—ã¦ãã ã•ã„ã€‚
                                            

    ===
    å‰æçŸ¥è­˜
    {context}

    ===
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•
    {question}
    """
    )
    # retriever = st.session_state.vectorstore.as_retriever(
    #     search_type="similarity"
    #     search_keywards={"k":10}
    # )
    
    text = st.session_state.textstore

    chain = (
        {"context": text, "question": RunnablePassthrough()}
        | prompt
        | llm
        |StrOutputParser()
    )
    return chain

def page_ask_my_pdf():
    select_model()
    chain = init_qa_chain()
    if query := st.text_input("PDFã¸ã®è³ªå•ã‚’æ›¸ã„ã¦ã­ï¼š ", key="input"):
        st.markdown("## Answer")
        st.write_stream(chain.stream(query))

def main():
    init_page()
    st.title("PDF QA")
    if "textstore" not in st.session_state:
        st.warning("ã¾ãšã¯Upload PDFã‹ã‚‰PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã­")
    else:
        page_ask_my_pdf()

if __name__ == '__main__':
    main()        